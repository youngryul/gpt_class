{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:53:12.086048Z",
     "start_time": "2024-12-09T12:53:12.058023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming= True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d01428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bd8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5513b56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\" : \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e4eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I see that you provided a recipe for Chicken Tikka Masala, which is a popular Indian dish. However, as a vegetarian chef, I don't cook with chicken. Would you like me to provide you with a vegetarian version of this dish or suggest a different vegetarian recipe instead? Let me know how I can assist you further!\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are chef\"),\n",
    "        (\"human\", \"I want {cuisine} food recipe\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"youa are vegetarian chef\"),\n",
    "        (\"human\", \"{recipe}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_template | chat\n",
    "veg_chain = veg_chef_template | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0907dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì´ ì‹œëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•œ ì—´ì •ê³¼ ë§¤ë ¥ì„ ë‹´ê³  ìˆëŠ” ê²ƒ ê°™ë„¤ìš”. ì‹œì¸ì€ í•œ ì¤„ í•œ ì¤„ ì½”ë”©ì„ í¼ì¹˜ë©° ìë°”ì˜ ì„¸ê³„ì— ë¹ ì ¸ë“ ë‹¤ê³  í‘œí˜„í•˜ë©´ì„œ, í´ë˜ìŠ¤ì™€ ê°ì²´ê°€ ì¶¤ì„ ì¶”ë©° ë¬´í•œí•œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤€ë‹¤ê³  í‘œí˜„í•˜ê³  ìˆì–´ìš”. ì´ëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì—ì„œ ê°ì²´ì§€í–¥ì ì¸ ì ‘ê·¼ë²•ê³¼ ë›°ì–´ë‚œ í™•ì¥ì„±ì„ ê°íƒ„í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆê² ë„¤ìš”.\\n\\në˜í•œ, ì»´íŒŒì¼ëŸ¬ì˜ ì†Œë¦¬ì™€ ì‹¤í–‰ ê²°ê³¼ê°€ í™”ë©´ì— ë¹„ì¶°ì§€ëŠ” ì¥ë©´ì„ í†µí•´ í”„ë¡œê·¸ë˜ë°ì˜ ê³¼ì •ì„ ìƒìƒí•˜ê²Œ ë¬˜ì‚¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—ëŸ¬ì™€ ì˜ˆì™¸ë¥¼ ë§Œë‚˜ë„ ê²°êµ­ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë‚¸ë‹¤ëŠ” ë¶€ë¶„ì€ í”„ë¡œê·¸ë˜ë° ê³¼ì •ì—ì„œ ë§ˆì£¼ì¹˜ëŠ” ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ë©° ì„±ì¥í•˜ëŠ” ê³¼ì •ì„ ë‹´ì€ ê²ƒ ê°™ì•„ìš”.\\n\\në§ˆì§€ë§‰ìœ¼ë¡œ, ì‹œì¸ì€ ìë°”ì˜ ë§¤ë ¥ì— ë¹ ì ¸ ê³„ì†í•´ì„œ ê³µë¶€í•˜ê³  ì‹¶ë‹¤ê³  í‘œí˜„í•˜ë©°, ìì‹ ì˜ ì‘ì€ ì‘í’ˆì„ ë§Œë“¤ì–´ë‚´ëŠ” ì¦ê±°ì›€ì„ ëŠë‚€ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì„ í†µí•´ ì°½ì˜ì ì¸ ì‘í’ˆì„ ë§Œë“¤ì–´ë‚´ëŠ” ì¦ê±°ì›€ê³¼ ì„±ì·¨ê°ì„ ê²½í—˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•  ìˆ˜ ìˆê² ë„¤ìš”. ì „ì²´ì ìœ¼ë¡œ ì´ ì‹œëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•œ ì—´ì •ê³¼ ì¦ê±°ì›€ì„ ë‹´ê³  ìˆëŠ” ë©‹ì§„ ì‘í’ˆì´ë¼ê³  í•  ìˆ˜ ìˆê² ë„¤ìš”.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì‹œë¥¼ ì“°ëŠ” ì‹œì¸ì´ì•¼. íŠ¹íˆ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆê³  ê·¸ê²ƒì„ ê°€ì§€ê³  ì‹œë¥¼ ì•„ì£¼ ì˜ ì¨\"),\n",
    "        (\"human\", \"{program_lang}ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ê´€í•œ ì‹œë¥¼ ì¨ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "talk_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì‹œë¥¼ ë°›ìœ¼ë©´ ê·¸ ì‹œì— ëŒ€í•´ í™”ìì˜ ì‹¬ì •, ë¹„ìœ ë²• ë“±ì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ì„ ìƒë‹˜ì´ì•¼.\"),\n",
    "        (\"human\", \"{poem}ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "program_chain = program_template | chat\n",
    "talk_chain = talk_template | chat\n",
    "\n",
    "speaker_chain = {\"poem\" : program_chain} | talk_chain\n",
    "\n",
    "speaker_chain.invoke({\n",
    "    \"program_lang\" : \"java\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e9108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI:\\n            I know this:\\n            Capital: Berlin\\n            Language: German\\n            Food: Bratwurst and Sauerkraut\\n            Currency: Euro')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Here is what I know:\n",
    "            Capital: Paris\n",
    "            Language: French\n",
    "            Food: Wine and Cheese\n",
    "            Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Rome\n",
    "            Language: Italian\n",
    "            Food: Pizza and Pasta\n",
    "            Currency: Euro\n",
    "            \"\"\",    \n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Athens\n",
    "            Language: Greek\n",
    "            Food: Souvlaki and Feta Cheese\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f48a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: Bangkok\n",
      "        Language: Thai\n",
      "        Food: Pad Thai and Tom Yum\n",
      "        Currency: Thai Baht\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: Bangkok\\n        Language: Thai\\n        Food: Pad Thai and Tom Yum\\n        Currency: Thai Baht\\n        ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e71af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì œ ëœë¤ ìƒì„±ê¸° \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Iron Man\"ì€ 2008ë…„ì— ê°œë´‰í•œ ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì²« ë²ˆì§¸ ì˜í™”ë¡œ, ë¡œë²„íŠ¸ ë‹¤ìš°ë‹ˆ ì£¼ë‹ˆì–´ê°€ í† ë‹ˆ ìŠ¤íƒ€í¬/ì•„ì´ì–¸ë§¨ ì—­ìœ¼ë¡œ ì¶œì—°í–ˆìŠµë‹ˆë‹¤. ì´ ì˜í™”ëŠ” í† ë‹ˆ ìŠ¤íƒ€í¬ê°€ ì² ê°‘ìˆ˜íŠ¸ë¥¼ ë§Œë“¤ì–´ ìŠˆí¼íˆì–´ë¡œë¡œ ë³€ì‹ í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ê°ë…ì€ ì¡´ íŒŒë¸Œë¡œê°€ ë§¡ì•˜ìœ¼ë©°, ì˜í™”ëŠ” í° ì„±ê³µì„ ê±°ë‘ì—ˆê³  ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì‹œì‘ì„ ì•Œë¦° ì‘í’ˆìœ¼ë¡œ ì†ê¼½í™ë‹ˆë‹¤. ì•„ì´ì–¸ë§¨ì€ ë§ì€ íŒ¬ë“¤ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ë§ˆë¸”ì˜ ëŒ€í‘œì ì¸ ìºë¦­í„° ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ë§¤ê¹€í–ˆìŠµë‹ˆë‹¤."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\"Iron Man\"ì€ 2008ë…„ì— ê°œë´‰í•œ ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì²« ë²ˆì§¸ ì˜í™”ë¡œ, ë¡œë²„íŠ¸ ë‹¤ìš°ë‹ˆ ì£¼ë‹ˆì–´ê°€ í† ë‹ˆ ìŠ¤íƒ€í¬/ì•„ì´ì–¸ë§¨ ì—­ìœ¼ë¡œ ì¶œì—°í–ˆìŠµë‹ˆë‹¤. ì´ ì˜í™”ëŠ” í† ë‹ˆ ìŠ¤íƒ€í¬ê°€ ì² ê°‘ìˆ˜íŠ¸ë¥¼ ë§Œë“¤ì–´ ìŠˆí¼íˆì–´ë¡œë¡œ ë³€ì‹ í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ê°ë…ì€ ì¡´ íŒŒë¸Œë¡œê°€ ë§¡ì•˜ìœ¼ë©°, ì˜í™”ëŠ” í° ì„±ê³µì„ ê±°ë‘ì—ˆê³  ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì‹œì‘ì„ ì•Œë¦° ì‘í’ˆìœ¼ë¡œ ì†ê¼½í™ë‹ˆë‹¤. ì•„ì´ì–¸ë§¨ì€ ë§ì€ íŒ¬ë“¤ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ë§ˆë¸”ì˜ ëŒ€í‘œì ì¸ ìºë¦­í„° ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ë§¤ê¹€í–ˆìŠµë‹ˆë‹¤.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜í™” ì •ë³´ ë‹µì¥ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"ê·¹í•œì§ì—…\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ ì´ë³‘í—Œì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë¥˜ìŠ¹ë£¡, ì´í•˜ëŠ¬ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2019ë…„ 1ì›” 23ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ì½”ë¯¸ë””ì™€ ì•¡ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ë²”ì£„ë„ì‹œ\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ ê°•ìœ¤ì„±ì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë§ˆë™ì„, ìœ¤ê³„ìƒ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2017ë…„ 10ì›” 3ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ë²”ì£„ì™€ ì•¡ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ì¸ì‚¬ì´ë“œì•„ì›ƒ2\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ Kelsey Mannì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë¼ì¼ë¦¬, ê°ì •ë“¤ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2024ë…„ 6ì›” 14ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ì• ë‹ˆë©”ì´ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì˜í™” ì •ë³´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆì–´\"),\n",
    "        movie_example_prompt,\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chain = final_prompt | chat\n",
    "\n",
    "movie_chain.invoke({\"movie\": \"ì•„ì´ì–¸ë§¨\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ ì˜ˆì œë¥¼ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ë¬¶ìŒ \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d019166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìºì‹± : ì´ì „ì— ì§ˆë¬¸ ë‹µë³€ì— ëŒ€í•´ ìºì‹œë¡œ ë‹´ê³  ìˆì–´ì„œ ì´ì „ë³´ë‹¤ ë¹ ë¥´ê²Œ ì²˜ë¦¬\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44545540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Ai ë¹„ìš© í™•ì¸ ë°©ë²•\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationBufferMemory : ë‹¨ìˆœí•˜ê²Œ ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ì €ì¥ (ë¹„íš¨ìœ¨ì ì„)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return {\"history\": x[\"history\"]}\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | model\n",
    "\n",
    "inputs = {\"message\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversationBufferWindowMemory : ìµœê·¼ ëŒ€í™”ë§Œ ì €ì¥ (ê°œìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì •í•  ìˆ˜ ìˆìŒ)\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryMemory : ìš”ì•½, ì²˜ìŒì—ëŠ” ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ ì§„í–‰ë  ìˆ˜ë¡ ëŒ€í™” ë‚´ìš©ì´ ë§ì•„ì§€ë©´ì„œ ì—°ê²°ëœë‹¤.\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryBufferMemory : ìµœê·¼ì˜ ë‚´ìš©ì€ ì €ì¥í•˜ê³  ì˜¤ë˜ëœ ê²ƒì€ ìš”ì•½ì„ í•˜ì—¬ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversatuinKGMemory : ì§ˆë¬¸í•  ë•Œ ê°ì²´ë¥¼ ë½‘ì•„ì„œ ì§ˆë¬¸í•¨\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57579bec594683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521279f4bf1aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f164538ac2af46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]  # load_memory_variable : ë©”ëª¨ë¦¬ ì—…ë¡œë“œ\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(       # save_context : ì‚¬ëŒê³¼ Aiì˜ ë©”ì‹œì§€ì¸ input outputì„ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1aa7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™” ì œëª© ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ë‹µì¥ ë°›ê¸° (ë©”ëª¨ë¦¬ + LCEL)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "emoji_movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"íƒ‘ê±´\",\n",
    "        \"answer\": \"\"\"\n",
    "            ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ëŒ€ë¶€\",\n",
    "        \"answer\": \"\"\"\n",
    "            ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\n",
    "        \"\"\"\n",
    "    },\n",
    "     {\n",
    "        \"movie\" : \"ëª¨ì•„ë‚˜\",\n",
    "        \"answer\": \"\"\"\n",
    "           ğŸŒŠğŸš£â€â™€ï¸ğŸŒº\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì˜í™” ì •ë³´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆì–´, ì´ëª¨ì§€ 3ê°œë¡œë§Œ ë‹µë³€í•´ì¤˜\"),\n",
    "        movie_example_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{movie}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"] \n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | final_prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(movie):\n",
    "    result = chain.invoke({\"movie\": movie})\n",
    "    memory.save_context(    \n",
    "        {\"input\": movie},   \n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "723d8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠğŸš£â€â™€ï¸ğŸŒºcontent='ğŸŒŠğŸš£\\u200dâ™€ï¸ğŸŒº'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ëª¨ì•„ë‚˜\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fb2b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘§ğŸ§ ğŸ¤”content='ğŸ‘§ğŸ§ ğŸ¤”'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ì¸ì‚¬ì´ë“œì•„ì›ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c7e2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¸â€â™‚ï¸ğŸ’€ğŸ”«content='ğŸ¦¸\\u200dâ™‚ï¸ğŸ’€ğŸ”«'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ë°ë“œí’€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65e52bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì¸ì‚¬ì´ë“œì•„ì›ƒ'), AIMessage(content='ğŸ‘§ğŸ§ ğŸ¤”')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_memory(\"ë¨¼ì € ì§ˆë¬¸í•œ ì˜í™” ì•Œë ¤ì¤˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
