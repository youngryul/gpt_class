{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:53:12.086048Z",
     "start_time": "2024-12-09T12:53:12.058023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming= True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d01428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bd8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system : ì£¼ë¡œ ë°°ì—´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”, ë‹¹ì‹ ì€ ì–´ë–¤ ì—­í• ì„ ë¶€ì—¬ë°›ì•˜ìŠµë‹ˆë‹¤ ë“±ìœ¼ë¡œ ëŒ€ë‹µì˜ í‹€ì„ ë§Œë“¤ì–´ì¤€ë‹¤.\n",
    "# human : ì‹¤ì œ ì§ˆë¬¸\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5513b56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputParser : AIë¡œë¶€í„° ë°›ì€ ì‘ë‹µì„ ë³€í˜•\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\" : \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e4eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I see that you provided a recipe for Chicken Tikka Masala, which is a popular Indian dish. However, as a vegetarian chef, I don't cook with chicken. Would you like me to provide you with a vegetarian version of this dish or suggest a different vegetarian recipe instead? Let me know how I can assist you further!\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are chef\"),\n",
    "        (\"human\", \"I want {cuisine} food recipe\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"youa are vegetarian chef\"),\n",
    "        (\"human\", \"{recipe}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_template | chat\n",
    "veg_chain = veg_chef_template | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0907dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì´ ì‹œëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•œ ì—´ì •ê³¼ ë§¤ë ¥ì„ ë‹´ê³  ìˆëŠ” ê²ƒ ê°™ë„¤ìš”. ì‹œì¸ì€ í•œ ì¤„ í•œ ì¤„ ì½”ë”©ì„ í¼ì¹˜ë©° ìë°”ì˜ ì„¸ê³„ì— ë¹ ì ¸ë“ ë‹¤ê³  í‘œí˜„í•˜ë©´ì„œ, í´ë˜ìŠ¤ì™€ ê°ì²´ê°€ ì¶¤ì„ ì¶”ë©° ë¬´í•œí•œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤€ë‹¤ê³  í‘œí˜„í•˜ê³  ìˆì–´ìš”. ì´ëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì—ì„œ ê°ì²´ì§€í–¥ì ì¸ ì ‘ê·¼ë²•ê³¼ ë›°ì–´ë‚œ í™•ì¥ì„±ì„ ê°íƒ„í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆê² ë„¤ìš”.\\n\\në˜í•œ, ì»´íŒŒì¼ëŸ¬ì˜ ì†Œë¦¬ì™€ ì‹¤í–‰ ê²°ê³¼ê°€ í™”ë©´ì— ë¹„ì¶°ì§€ëŠ” ì¥ë©´ì„ í†µí•´ í”„ë¡œê·¸ë˜ë°ì˜ ê³¼ì •ì„ ìƒìƒí•˜ê²Œ ë¬˜ì‚¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—ëŸ¬ì™€ ì˜ˆì™¸ë¥¼ ë§Œë‚˜ë„ ê²°êµ­ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë‚¸ë‹¤ëŠ” ë¶€ë¶„ì€ í”„ë¡œê·¸ë˜ë° ê³¼ì •ì—ì„œ ë§ˆì£¼ì¹˜ëŠ” ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ë©° ì„±ì¥í•˜ëŠ” ê³¼ì •ì„ ë‹´ì€ ê²ƒ ê°™ì•„ìš”.\\n\\në§ˆì§€ë§‰ìœ¼ë¡œ, ì‹œì¸ì€ ìë°”ì˜ ë§¤ë ¥ì— ë¹ ì ¸ ê³„ì†í•´ì„œ ê³µë¶€í•˜ê³  ì‹¶ë‹¤ê³  í‘œí˜„í•˜ë©°, ìì‹ ì˜ ì‘ì€ ì‘í’ˆì„ ë§Œë“¤ì–´ë‚´ëŠ” ì¦ê±°ì›€ì„ ëŠë‚€ë‹¤ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì„ í†µí•´ ì°½ì˜ì ì¸ ì‘í’ˆì„ ë§Œë“¤ì–´ë‚´ëŠ” ì¦ê±°ì›€ê³¼ ì„±ì·¨ê°ì„ ê²½í—˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•  ìˆ˜ ìˆê² ë„¤ìš”. ì „ì²´ì ìœ¼ë¡œ ì´ ì‹œëŠ” ìë°” í”„ë¡œê·¸ë˜ë°ì— ëŒ€í•œ ì—´ì •ê³¼ ì¦ê±°ì›€ì„ ë‹´ê³  ìˆëŠ” ë©‹ì§„ ì‘í’ˆì´ë¼ê³  í•  ìˆ˜ ìˆê² ë„¤ìš”.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì‹œë¥¼ ì“°ëŠ” ì‹œì¸ì´ì•¼. íŠ¹íˆ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆê³  ê·¸ê²ƒì„ ê°€ì§€ê³  ì‹œë¥¼ ì•„ì£¼ ì˜ ì¨\"),\n",
    "        (\"human\", \"{program_lang}ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ê´€í•œ ì‹œë¥¼ ì¨ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "talk_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì‹œë¥¼ ë°›ìœ¼ë©´ ê·¸ ì‹œì— ëŒ€í•´ í™”ìì˜ ì‹¬ì •, ë¹„ìœ ë²• ë“±ì— ëŒ€í•´ ì„¤ëª…í•˜ëŠ” ì„ ìƒë‹˜ì´ì•¼.\"),\n",
    "        (\"human\", \"{poem}ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "program_chain = program_template | chat\n",
    "talk_chain = talk_template | chat\n",
    "\n",
    "speaker_chain = {\"poem\" : program_chain} | talk_chain\n",
    "\n",
    "speaker_chain.invoke({\n",
    "    \"program_lang\" : \"java\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e9108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI:\\n            I know this:\\n            Capital: Berlin\\n            Language: German\\n            Food: Bratwurst and Sauerkraut\\n            Currency: Euro')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Here is what I know:\n",
    "            Capital: Paris\n",
    "            Language: French\n",
    "            Food: Wine and Cheese\n",
    "            Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Rome\n",
    "            Language: Italian\n",
    "            Food: Pizza and Pasta\n",
    "            Currency: Euro\n",
    "            \"\"\",    \n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Athens\n",
    "            Language: Greek\n",
    "            Food: Souvlaki and Feta Cheese\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f48a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: Bangkok\n",
      "        Language: Thai\n",
      "        Food: Pad Thai and Tom Yum\n",
      "        Currency: Thai Baht\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: Bangkok\\n        Language: Thai\\n        Food: Pad Thai and Tom Yum\\n        Currency: Thai Baht\\n        ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e71af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì œ ëœë¤ ìƒì„±ê¸° \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Iron Man\"ì€ 2008ë…„ì— ê°œë´‰í•œ ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì²« ë²ˆì§¸ ì˜í™”ë¡œ, ë¡œë²„íŠ¸ ë‹¤ìš°ë‹ˆ ì£¼ë‹ˆì–´ê°€ í† ë‹ˆ ìŠ¤íƒ€í¬/ì•„ì´ì–¸ë§¨ ì—­ìœ¼ë¡œ ì¶œì—°í–ˆìŠµë‹ˆë‹¤. ì´ ì˜í™”ëŠ” í† ë‹ˆ ìŠ¤íƒ€í¬ê°€ ì² ê°‘ìˆ˜íŠ¸ë¥¼ ë§Œë“¤ì–´ ìŠˆí¼íˆì–´ë¡œë¡œ ë³€ì‹ í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ê°ë…ì€ ì¡´ íŒŒë¸Œë¡œê°€ ë§¡ì•˜ìœ¼ë©°, ì˜í™”ëŠ” í° ì„±ê³µì„ ê±°ë‘ì—ˆê³  ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì‹œì‘ì„ ì•Œë¦° ì‘í’ˆìœ¼ë¡œ ì†ê¼½í™ë‹ˆë‹¤. ì•„ì´ì–¸ë§¨ì€ ë§ì€ íŒ¬ë“¤ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ë§ˆë¸”ì˜ ëŒ€í‘œì ì¸ ìºë¦­í„° ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ë§¤ê¹€í–ˆìŠµë‹ˆë‹¤."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\"Iron Man\"ì€ 2008ë…„ì— ê°œë´‰í•œ ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì²« ë²ˆì§¸ ì˜í™”ë¡œ, ë¡œë²„íŠ¸ ë‹¤ìš°ë‹ˆ ì£¼ë‹ˆì–´ê°€ í† ë‹ˆ ìŠ¤íƒ€í¬/ì•„ì´ì–¸ë§¨ ì—­ìœ¼ë¡œ ì¶œì—°í–ˆìŠµë‹ˆë‹¤. ì´ ì˜í™”ëŠ” í† ë‹ˆ ìŠ¤íƒ€í¬ê°€ ì² ê°‘ìˆ˜íŠ¸ë¥¼ ë§Œë“¤ì–´ ìŠˆí¼íˆì–´ë¡œë¡œ ë³€ì‹ í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ê°ë…ì€ ì¡´ íŒŒë¸Œë¡œê°€ ë§¡ì•˜ìœ¼ë©°, ì˜í™”ëŠ” í° ì„±ê³µì„ ê±°ë‘ì—ˆê³  ë§ˆë¸” ì‹œë„¤ë§ˆí‹± ìœ ë‹ˆë²„ìŠ¤ì˜ ì‹œì‘ì„ ì•Œë¦° ì‘í’ˆìœ¼ë¡œ ì†ê¼½í™ë‹ˆë‹¤. ì•„ì´ì–¸ë§¨ì€ ë§ì€ íŒ¬ë“¤ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ë§ˆë¸”ì˜ ëŒ€í‘œì ì¸ ìºë¦­í„° ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ë§¤ê¹€í–ˆìŠµë‹ˆë‹¤.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜í™” ì •ë³´ ë‹µì¥ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"ê·¹í•œì§ì—…\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ ì´ë³‘í—Œì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë¥˜ìŠ¹ë£¡, ì´í•˜ëŠ¬ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2019ë…„ 1ì›” 23ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ì½”ë¯¸ë””ì™€ ì•¡ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ë²”ì£„ë„ì‹œ\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ ê°•ìœ¤ì„±ì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë§ˆë™ì„, ìœ¤ê³„ìƒ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2017ë…„ 10ì›” 3ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ë²”ì£„ì™€ ì•¡ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ì¸ì‚¬ì´ë“œì•„ì›ƒ2\",\n",
    "        \"answer\": \"\"\"\n",
    "            ê°ë…ì€ Kelsey Mannì…ë‹ˆë‹¤.\n",
    "            ì£¼ìš” ì¶œì—°ì§„ì€ ë¼ì¼ë¦¬, ê°ì •ë“¤ ë“±ì´ ì¶œì—°í•©ë‹ˆë‹¤.\n",
    "            ê°œë´‰ì¼ì€ 2024ë…„ 6ì›” 14ì¼ì…ë‹ˆë‹¤.\n",
    "            ì¥ë¥´ëŠ” ì• ë‹ˆë©”ì´ì…˜ì…ë‹ˆë‹¤. \n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì˜í™” ì •ë³´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆì–´\"),\n",
    "        movie_example_prompt,\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chain = final_prompt | chat\n",
    "\n",
    "movie_chain.invoke({\"movie\": \"ì•„ì´ì–¸ë§¨\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ ì˜ˆì œë¥¼ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ë¬¶ìŒ \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d019166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìºì‹± : ì´ì „ì— ì§ˆë¬¸ ë‹µë³€ì— ëŒ€í•´ ìºì‹œë¡œ ë‹´ê³  ìˆì–´ì„œ ì´ì „ë³´ë‹¤ ë¹ ë¥´ê²Œ ì²˜ë¦¬\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44545540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Ai ë¹„ìš© í™•ì¸ ë°©ë²•\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationBufferMemory : ë‹¨ìˆœí•˜ê²Œ ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ì €ì¥ (ë¹„íš¨ìœ¨ì ì„)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return {\"history\": x[\"history\"]}\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | model\n",
    "\n",
    "inputs = {\"message\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversationBufferWindowMemory : ìµœê·¼ ëŒ€í™”ë§Œ ì €ì¥ (ê°œìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì •í•  ìˆ˜ ìˆìŒ)\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryMemory : ìš”ì•½, ì²˜ìŒì—ëŠ” ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ ì§„í–‰ë  ìˆ˜ë¡ ëŒ€í™” ë‚´ìš©ì´ ë§ì•„ì§€ë©´ì„œ ì—°ê²°ëœë‹¤.\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryBufferMemory : ìµœê·¼ì˜ ë‚´ìš©ì€ ì €ì¥í•˜ê³  ì˜¤ë˜ëœ ê²ƒì€ ìš”ì•½ì„ í•˜ì—¬ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversatuinKGMemory : ì§ˆë¬¸í•  ë•Œ ê°ì²´ë¥¼ ë½‘ì•„ì„œ ì§ˆë¬¸í•¨\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57579bec594683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521279f4bf1aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f164538ac2af46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]  # load_memory_variable : ë©”ëª¨ë¦¬ ì—…ë¡œë“œ\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(       # save_context : ì‚¬ëŒê³¼ Aiì˜ ë©”ì‹œì§€ì¸ input outputì„ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1aa7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜í™” ì œëª© ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ë‹µì¥ ë°›ê¸° (ë©”ëª¨ë¦¬ + LCEL)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "emoji_movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"íƒ‘ê±´\",\n",
    "        \"answer\": \"\"\"\n",
    "            ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"ëŒ€ë¶€\",\n",
    "        \"answer\": \"\"\"\n",
    "            ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\n",
    "        \"\"\"\n",
    "    },\n",
    "     {\n",
    "        \"movie\" : \"ëª¨ì•„ë‚˜\",\n",
    "        \"answer\": \"\"\"\n",
    "           ğŸŒŠğŸš£â€â™€ï¸ğŸŒº\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}ì— ëŒ€í•œ ì˜í™” ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì˜í™” ì •ë³´ì— ëŒ€í•´ ì˜ ì•Œê³  ìˆì–´, ì´ëª¨ì§€ 3ê°œë¡œë§Œ ë‹µë³€í•´ì¤˜\"),\n",
    "        movie_example_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{movie}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"] \n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | final_prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(movie):\n",
    "    result = chain.invoke({\"movie\": movie})\n",
    "    memory.save_context(    \n",
    "        {\"input\": movie},   \n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "723d8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠğŸš£â€â™€ï¸ğŸŒºcontent='ğŸŒŠğŸš£\\u200dâ™€ï¸ğŸŒº'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ëª¨ì•„ë‚˜\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fb2b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘§ğŸ§ ğŸ¤”content='ğŸ‘§ğŸ§ ğŸ¤”'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ì¸ì‚¬ì´ë“œì•„ì›ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c7e2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¸â€â™‚ï¸ğŸ’€ğŸ”«content='ğŸ¦¸\\u200dâ™‚ï¸ğŸ’€ğŸ”«'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"ë°ë“œí’€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65e52bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì¸ì‚¬ì´ë“œì•„ì›ƒ'), AIMessage(content='ğŸ‘§ğŸ§ ğŸ¤”')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_memory(\"ë¨¼ì € ì§ˆë¬¸í•œ ì˜í™” ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a99e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 717, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 1251, which is longer than the specified 600\n",
      "Created a chunk of size 1012, which is longer than the specified 600\n",
      "Created a chunk of size 1493, which is longer than the specified 600\n",
      "Created a chunk of size 819, which is longer than the specified 600\n",
      "Created a chunk of size 1458, which is longer than the specified 600\n",
      "Created a chunk of size 1411, which is longer than the specified 600\n",
      "Created a chunk of size 742, which is longer than the specified 600\n",
      "Created a chunk of size 669, which is longer than the specified 600\n",
      "Created a chunk of size 906, which is longer than the specified 600\n",
      "Created a chunk of size 703, which is longer than the specified 600\n",
      "Created a chunk of size 1137, which is longer than the specified 600\n",
      "Created a chunk of size 1417, which is longer than the specified 600\n",
      "Created a chunk of size 1200, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 845, which is longer than the specified 600\n",
      "Created a chunk of size 716, which is longer than the specified 600\n",
      "Created a chunk of size 840, which is longer than the specified 600\n",
      "Created a chunk of size 1042, which is longer than the specified 600\n",
      "Created a chunk of size 652, which is longer than the specified 600\n",
      "Created a chunk of size 985, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 659, which is longer than the specified 600\n",
      "Created a chunk of size 693, which is longer than the specified 600\n",
      "Created a chunk of size 817, which is longer than the specified 600\n",
      "Created a chunk of size 655, which is longer than the specified 600\n",
      "Created a chunk of size 1345, which is longer than the specified 600\n",
      "Created a chunk of size 1339, which is longer than the specified 600\n",
      "Created a chunk of size 1288, which is longer than the specified 600\n",
      "Created a chunk of size 1014, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 1178, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 802, which is longer than the specified 600\n",
      "Created a chunk of size 1496, which is longer than the specified 600\n",
      "Created a chunk of size 841, which is longer than the specified 600\n",
      "Created a chunk of size 743, which is longer than the specified 600\n",
      "Created a chunk of size 694, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Victory Mansions is a building where Winston Smith lives in George Orwell\\'s novel \"1984.\" It is a dilapidated apartment complex in a totalitarian society controlled by the Party. The building is described as run-down, with peeling paint, faulty plumbing, and an overall sense of neglect and decay. Despite its name, Victory Mansions do not provide a sense of victory or comfort to its residents, reflecting the oppressive and austere nature of the society in which they live.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG : retrieval augmented generation ê²€ìƒ‰-ì¦ê°• ìƒì„±, ëª¨ë¸ì´ ì´ë¯¸ í•™ìŠµí•œ ë°ì´í„° ì´ì™¸ì˜ ë°ì´í„°ë¥¼ ì œê³µí•´ì„œ ì¢€ ë” ì •êµí•œ ë‹µë³€ì„ ì¤€ë‹¤.\n",
    "# Unstructured File : txt, ppt, html ë“±ì„ í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” loader\n",
    "# recuriveCharacterTextSplitter, characterTextSplitter\n",
    "\n",
    "# load -> transform (split) -> embed -> store (ë°±í„° ë°ì´í„°ë² ì´ìŠ¤) -> retrieve\n",
    "\n",
    "# embed : ë¬¸ìë“¤ì„ ìˆ«ìë¡œ ì¹˜í™˜í•˜ì—¬ ì°¨ì›(ë°±í„°)ì„ ê°€ì§€ê³  ê³„ì‚°í•œë‹¤. (ì—°ê´€ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. : ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ)\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# off-the-shelf : ì´ë¯¸ ë§Œë“¤ì–´ì§„ ì²´ì¸ \n",
    "# chain_type : refine(ì •ì œ) ì—¬ëŸ¬ê°œì˜ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì—¬ ì•ˆì—ì„œ ë˜ ì§ˆë¬¸ì„ í•´ì„œ ì—…ë°ì´íŠ¸ ì‹œì¼œì¤€ë‹¤.\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d728a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Under the table Winston's feet made convulsive movements. He had not stirred from his seat, but in his mind he was running, swiftly running, he was with the crowds outside, cheering himself deaf. He looked up again at the portrait of Big Brother. The colossus that bestrode the world! The rock against which the hordes of Asia dashed themselves in vain! He thought how ten minutes ago--yes, only ten minutes--there had still been equivocation in his heart as he wondered whether the news from the front would be of victory or defeat. Ah, it was more than a Eurasian army that had perished! Much had changed in him since that first day in the Ministry of Love, but the final, indispensable, healing change had never happened, until this moment.\", metadata={'source': './files/document.txt'}),\n",
       " Document(page_content=\"Winston thought. 'By making him suffer,' he said.\", metadata={'source': './files/document.txt'}),\n",
       " Document(page_content='filled his glass up with Victory Gin, shaking into it a few drops from\\nanother bottle with a quill through the cork. It was saccharine flavoured\\nwith cloves, the speciality of the cafe.\\nWinston was listening to the telescreen. At present only music was coming\\nout of it, but there was a possibility that at any moment there might be\\na special bulletin from the Ministry of Peace. The news from the African\\nfront was disquieting in the extreme. On and off he had been worrying\\nabout it all day. A Eurasian army (Oceania was at war with Eurasia:', metadata={'source': './files/document.txt'}),\n",
       " Document(page_content='another metallic click, and knew that the cage door had clicked shut and\\nnot open.\\nChapter 6\\nThe Chestnut Tree was almost empty. A ray of sunlight slanting through a\\nwindow fell on dusty table-tops. It was the lonely hour of fifteen. A\\ntinny music trickled from the telescreens.\\nWinston sat in his usual corner, gazing into an empty glass. Now and again\\nhe glanced up at a vast face which eyed him from the opposite wall.\\nBIG BROTHER IS WATCHING YOU, the caption said. Unbidden, a waiter came and\\nfilled his glass up with Victory Gin, shaking into it a few drops from', metadata={'source': './files/document.txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"where does winston live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567e0e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9685850> JSON: {\n",
       "  \"prompt_tokens\": 2122,\n",
       "  \"completion_tokens\": 20,\n",
       "  \"total_tokens\": 2142,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-76b78b5e-d6bd-4521-8fd7-8b04bb44edb2-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "         MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return x[\"history\"]\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(history=load_memory)\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Is Aaronson guilty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e11ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='He wrote \"2+2=5\" in the dust on the table.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9603170> JSON: {\n",
       "  \"prompt_tokens\": 2220,\n",
       "  \"completion_tokens\": 16,\n",
       "  \"total_tokens\": 2236,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8e510fa9-bdce-49eb-9af7-8ed9bc7905a6-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What message did he write in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303cbfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Julia is a character mentioned in the text. She is someone who Winston loves and cares for deeply, and he expresses a strong desire to protect her from harm.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9684b30> JSON: {\n",
       "  \"prompt_tokens\": 2074,\n",
       "  \"completion_tokens\": 33,\n",
       "  \"total_tokens\": 2107,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-83fef15b-5c91-44a0-b7e5-cf2180c26534-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a93d0305233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "                \"documents\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            } | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer.\n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind( # autoë¡œ ì£¼ë©´ ì•Œì•„ì„œ ê²°ì •í•´ì„œ ì”€ ë‚´ê°€ ë§Œë“  ê±¸ ì“¸ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„ ìˆìŒ\n",
    "    function_call={\n",
    "        \"name\": \"create_quiz\",\n",
    "    },\n",
    "    functions=[\n",
    "        function,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "response"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[\n",
    "        StructuredTool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. This tool take two arguments, both  should be numbers.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "555f75e51314322e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(inputs):\n",
    "    a, b = inputs.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample:1,2\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "90e5c3f784946479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float = Field(description=\"The first number\")\n",
    "    b: float = Field(description=\"The second number\")\n",
    "\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"CalculatorTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CalculatorToolArgsSchema] = CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "1ff01a749e2e815d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
