{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:53:12.086048Z",
     "start_time": "2024-12-09T12:53:12.058023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming= True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d01428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bd8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system : 주로 배열로 대답하세요, 당신은 어떤 역할을 부여받았습니다 등으로 대답의 틀을 만들어준다.\n",
    "# human : 실제 질문\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items} in lowercase.Do NOT reply with anything else.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5513b56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charmander', 'bulbasaur', 'squirtle', 'jigglypuff']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputParser : AI로부터 받은 응답을 변형\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\" : \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e4eb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I see that you provided a recipe for Chicken Tikka Masala, which is a popular Indian dish. However, as a vegetarian chef, I don't cook with chicken. Would you like me to provide you with a vegetarian version of this dish or suggest a different vegetarian recipe instead? Let me know how I can assist you further!\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are chef\"),\n",
    "        (\"human\", \"I want {cuisine} food recipe\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "veg_chef_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"youa are vegetarian chef\"),\n",
    "        (\"human\", \"{recipe}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_template | chat\n",
    "veg_chain = veg_chef_template | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"cuisine\": \"indian\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0907dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='이 시는 자바 프로그래밍에 대한 열정과 매력을 담고 있는 것 같네요. 시인은 한 줄 한 줄 코딩을 펼치며 자바의 세계에 빠져든다고 표현하면서, 클래스와 객체가 춤을 추며 무한한 가능성을 보여준다고 표현하고 있어요. 이는 자바 프로그래밍에서 객체지향적인 접근법과 뛰어난 확장성을 감탄하는 것으로 해석될 수 있겠네요.\\n\\n또한, 컴파일러의 소리와 실행 결과가 화면에 비춰지는 장면을 통해 프로그래밍의 과정을 생생하게 묘사하고 있습니다. 에러와 예외를 만나도 결국은 해결책을 찾아낸다는 부분은 프로그래밍 과정에서 마주치는 어려움을 극복하며 성장하는 과정을 담은 것 같아요.\\n\\n마지막으로, 시인은 자바의 매력에 빠져 계속해서 공부하고 싶다고 표현하며, 자신의 작은 작품을 만들어내는 즐거움을 느낀다고 말하고 있습니다. 이는 자바 프로그래밍을 통해 창의적인 작품을 만들어내는 즐거움과 성취감을 경험하는 것을 의미할 수 있겠네요. 전체적으로 이 시는 자바 프로그래밍에 대한 열정과 즐거움을 담고 있는 멋진 작품이라고 할 수 있겠네요.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 시를 쓰는 시인이야. 특히 프로그래밍 언어에 대해 잘 알고 있고 그것을 가지고 시를 아주 잘 써\"),\n",
    "        (\"human\", \"{program_lang}으로 프로그래밍 언어에 관한 시를 써줘\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "talk_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 시를 받으면 그 시에 대해 화자의 심정, 비유법 등에 대해 설명하는 선생님이야.\"),\n",
    "        (\"human\", \"{poem}에 대해 설명해줘\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "program_chain = program_template | chat\n",
    "talk_chain = talk_template | chat\n",
    "\n",
    "speaker_chain = {\"poem\" : program_chain} | talk_chain\n",
    "\n",
    "speaker_chain.invoke({\n",
    "    \"program_lang\" : \"java\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e9108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI:\\n            I know this:\\n            Capital: Berlin\\n            Language: German\\n            Food: Bratwurst and Sauerkraut\\n            Currency: Euro')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "            Here is what I know:\n",
    "            Capital: Paris\n",
    "            Language: French\n",
    "            Food: Wine and Cheese\n",
    "            Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Rome\n",
    "            Language: Italian\n",
    "            Food: Pizza and Pasta\n",
    "            Currency: Euro\n",
    "            \"\"\",    \n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "            I know this:\n",
    "            Capital: Athens\n",
    "            Language: Greek\n",
    "            Food: Souvlaki and Feta Cheese\n",
    "            Currency: Euro\n",
    "            \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f48a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: Bangkok\n",
      "        Language: Thai\n",
      "        Food: Pad Thai and Tom Yum\n",
      "        Currency: Thai Baht\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: Bangkok\\n        Language: Thai\\n        Food: Pad Thai and Tom Yum\\n        Currency: Thai Baht\\n        ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert, you give short answers.\"),\n",
    "        example_prompt,\n",
    "        (\"human\", \"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Thailand\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e71af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 랜덤 생성기 \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Iron Man\"은 2008년에 개봉한 마블 시네마틱 유니버스의 첫 번째 영화로, 로버트 다우니 주니어가 토니 스타크/아이언맨 역으로 출연했습니다. 이 영화는 토니 스타크가 철갑수트를 만들어 슈퍼히어로로 변신하는 이야기를 다루고 있습니다. 감독은 존 파브로가 맡았으며, 영화는 큰 성공을 거두었고 마블 시네마틱 유니버스의 시작을 알린 작품으로 손꼽힙니다. 아이언맨은 많은 팬들에게 사랑받는 마블의 대표적인 캐릭터 중 하나로 자리매김했습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\"Iron Man\"은 2008년에 개봉한 마블 시네마틱 유니버스의 첫 번째 영화로, 로버트 다우니 주니어가 토니 스타크/아이언맨 역으로 출연했습니다. 이 영화는 토니 스타크가 철갑수트를 만들어 슈퍼히어로로 변신하는 이야기를 다루고 있습니다. 감독은 존 파브로가 맡았으며, 영화는 큰 성공을 거두었고 마블 시네마틱 유니버스의 시작을 알린 작품으로 손꼽힙니다. 아이언맨은 많은 팬들에게 사랑받는 마블의 대표적인 캐릭터 중 하나로 자리매김했습니다.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화 정보 답장 프롬프트\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"극한직업\",\n",
    "        \"answer\": \"\"\"\n",
    "            감독은 이병헌입니다.\n",
    "            주요 출연진은 류승룡, 이하늬 등이 출연합니다.\n",
    "            개봉일은 2019년 1월 23일입니다.\n",
    "            장르는 코미디와 액션입니다. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"범죄도시\",\n",
    "        \"answer\": \"\"\"\n",
    "            감독은 강윤성입니다.\n",
    "            주요 출연진은 마동석, 윤계상 등이 출연합니다.\n",
    "            개봉일은 2017년 10월 3일입니다.\n",
    "            장르는 범죄와 액션입니다. \n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"인사이드아웃2\",\n",
    "        \"answer\": \"\"\"\n",
    "            감독은 Kelsey Mann입니다.\n",
    "            주요 출연진은 라일리, 감정들 등이 출연합니다.\n",
    "            개봉일은 2024년 6월 14일입니다.\n",
    "            장르는 애니메이션입니다. \n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}에 대한 영화 정보를 알려줘\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 영화 정보에 대해 잘 알고 있어\"),\n",
    "        movie_example_prompt,\n",
    "        (\"human\", \"{movie}에 대한 영화 정보를 알려줘\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chain = final_prompt | chat\n",
    "\n",
    "movie_chain.invoke({\"movie\": \"아이언맨\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 예제를 하나의 체인으로 묶음 \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\": \"Pirate\",\n",
    "        \"example_question\": \"What is your location?\",\n",
    "        \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "        \"question\": \"What is your fav food?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d019166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐싱 : 이전에 질문 답변에 대해 캐시로 담고 있어서 이전보다 빠르게 처리\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44545540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Ai 비용 확인 방법\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for soju\")\n",
    "    b = chat.predict(\"What is the recipe for bread\")\n",
    "    print(a, \"\\n\")\n",
    "    print(b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5cdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationBufferMemory : 단순하게 모든 대화 내용을 저장 (비효율적임)\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return {\"history\": x[\"history\"]}\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | model\n",
    "\n",
    "inputs = {\"message\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversationBufferWindowMemory : 최근 대화만 저장 (개수는 사용자가 정할 수 있음)\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryMemory : 요약, 처음에는 오래 걸리지만 진행될 수록 대화 내용이 많아지면서 연결된다.\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversationSummaryBufferMemory : 최근의 내용은 저장하고 오래된 것은 요약을 하여 사용한다.\n",
    "\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversatuinKGMemory : 질문할 때 객체를 뽑아서 질문함\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})\n",
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")\n",
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57579bec594683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521279f4bf1aaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f164538ac2af46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]  # load_memory_variable : 메모리 업로드\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(       # save_context : 사람과 Ai의 메시지인 input output을 메모리에 저장\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f1aa7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 제목 이모티콘으로 답장 받기 (메모리 + LCEL)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "emoji_movie_examples = [\n",
    "    {\n",
    "        \"movie\" : \"탑건\",\n",
    "        \"answer\": \"\"\"\n",
    "            🛩️👨‍✈️🔥\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"movie\" : \"대부\",\n",
    "        \"answer\": \"\"\"\n",
    "            👨‍👨‍👦🔫🍝\n",
    "        \"\"\"\n",
    "    },\n",
    "     {\n",
    "        \"movie\" : \"모아나\",\n",
    "        \"answer\": \"\"\"\n",
    "           🌊🚣‍♀️🌺\n",
    "        \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "movie_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{movie}에 대한 영화 정보를 알려줘\"),\n",
    "        (\"ai\", \"{answer}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=movie_example_prompt,\n",
    "    examples=emoji_movie_examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 영화 정보에 대해 잘 알고 있어, 이모지 3개로만 답변해줘\"),\n",
    "        movie_example_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{movie}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"] \n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | final_prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(movie):\n",
    "    result = chain.invoke({\"movie\": movie})\n",
    "    memory.save_context(    \n",
    "        {\"input\": movie},   \n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "723d8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊🚣‍♀️🌺content='🌊🚣\\u200d♀️🌺'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"모아나\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fb2b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👧🧠🤔content='👧🧠🤔'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"인사이드아웃\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c7e2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦸‍♂️💀🔫content='🦸\\u200d♂️💀🔫'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"데드풀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65e52bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='인사이드아웃'), AIMessage(content='👧🧠🤔')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_memory(\"먼저 질문한 영화 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a99e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 717, which is longer than the specified 600\n",
      "Created a chunk of size 608, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 1251, which is longer than the specified 600\n",
      "Created a chunk of size 1012, which is longer than the specified 600\n",
      "Created a chunk of size 1493, which is longer than the specified 600\n",
      "Created a chunk of size 819, which is longer than the specified 600\n",
      "Created a chunk of size 1458, which is longer than the specified 600\n",
      "Created a chunk of size 1411, which is longer than the specified 600\n",
      "Created a chunk of size 742, which is longer than the specified 600\n",
      "Created a chunk of size 669, which is longer than the specified 600\n",
      "Created a chunk of size 906, which is longer than the specified 600\n",
      "Created a chunk of size 703, which is longer than the specified 600\n",
      "Created a chunk of size 1137, which is longer than the specified 600\n",
      "Created a chunk of size 1417, which is longer than the specified 600\n",
      "Created a chunk of size 1200, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 845, which is longer than the specified 600\n",
      "Created a chunk of size 716, which is longer than the specified 600\n",
      "Created a chunk of size 840, which is longer than the specified 600\n",
      "Created a chunk of size 1042, which is longer than the specified 600\n",
      "Created a chunk of size 652, which is longer than the specified 600\n",
      "Created a chunk of size 985, which is longer than the specified 600\n",
      "Created a chunk of size 859, which is longer than the specified 600\n",
      "Created a chunk of size 659, which is longer than the specified 600\n",
      "Created a chunk of size 693, which is longer than the specified 600\n",
      "Created a chunk of size 817, which is longer than the specified 600\n",
      "Created a chunk of size 655, which is longer than the specified 600\n",
      "Created a chunk of size 1345, which is longer than the specified 600\n",
      "Created a chunk of size 1339, which is longer than the specified 600\n",
      "Created a chunk of size 1288, which is longer than the specified 600\n",
      "Created a chunk of size 1014, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 1178, which is longer than the specified 600\n",
      "Created a chunk of size 1444, which is longer than the specified 600\n",
      "Created a chunk of size 802, which is longer than the specified 600\n",
      "Created a chunk of size 1496, which is longer than the specified 600\n",
      "Created a chunk of size 841, which is longer than the specified 600\n",
      "Created a chunk of size 743, which is longer than the specified 600\n",
      "Created a chunk of size 694, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Victory Mansions is a building where Winston Smith lives in George Orwell\\'s novel \"1984.\" It is a dilapidated apartment complex in a totalitarian society controlled by the Party. The building is described as run-down, with peeling paint, faulty plumbing, and an overall sense of neglect and decay. Despite its name, Victory Mansions do not provide a sense of victory or comfort to its residents, reflecting the oppressive and austere nature of the society in which they live.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG : retrieval augmented generation 검색-증강 생성, 모델이 이미 학습한 데이터 이외의 데이터를 제공해서 좀 더 정교한 답변을 준다.\n",
    "# Unstructured File : txt, ppt, html 등을 한번에 처리할 수 있는 loader\n",
    "# recuriveCharacterTextSplitter, characterTextSplitter\n",
    "\n",
    "# load -> transform (split) -> embed -> store (백터 데이터베이스) -> retrieve\n",
    "\n",
    "# embed : 문자들을 숫자로 치환하여 차원(백터)을 가지고 계산한다. (연관성이 높은 단어를 찾을 수 있다. : 영화 추천 시스템)\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# off-the-shelf : 이미 만들어진 체인 \n",
    "# chain_type : refine(정제) 여러개의 문서를 확인하여 안에서 또 질문을 해서 업데이트 시켜준다.\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d728a5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Under the table Winston's feet made convulsive movements. He had not stirred from his seat, but in his mind he was running, swiftly running, he was with the crowds outside, cheering himself deaf. He looked up again at the portrait of Big Brother. The colossus that bestrode the world! The rock against which the hordes of Asia dashed themselves in vain! He thought how ten minutes ago--yes, only ten minutes--there had still been equivocation in his heart as he wondered whether the news from the front would be of victory or defeat. Ah, it was more than a Eurasian army that had perished! Much had changed in him since that first day in the Ministry of Love, but the final, indispensable, healing change had never happened, until this moment.\", metadata={'source': './files/document.txt'}),\n",
       " Document(page_content=\"Winston thought. 'By making him suffer,' he said.\", metadata={'source': './files/document.txt'}),\n",
       " Document(page_content='filled his glass up with Victory Gin, shaking into it a few drops from\\nanother bottle with a quill through the cork. It was saccharine flavoured\\nwith cloves, the speciality of the cafe.\\nWinston was listening to the telescreen. At present only music was coming\\nout of it, but there was a possibility that at any moment there might be\\na special bulletin from the Ministry of Peace. The news from the African\\nfront was disquieting in the extreme. On and off he had been worrying\\nabout it all day. A Eurasian army (Oceania was at war with Eurasia:', metadata={'source': './files/document.txt'}),\n",
       " Document(page_content='another metallic click, and knew that the cage door had clicked shut and\\nnot open.\\nChapter 6\\nThe Chestnut Tree was almost empty. A ray of sunlight slanting through a\\nwindow fell on dusty table-tops. It was the lonely hour of fifteen. A\\ntinny music trickled from the telescreens.\\nWinston sat in his usual corner, gazing into an empty glass. Now and again\\nhe glanced up at a vast face which eyed him from the opposite wall.\\nBIG BROTHER IS WATCHING YOU, the caption said. Unbidden, a waiter came and\\nfilled his glass up with Victory Gin, shaking into it a few drops from', metadata={'source': './files/document.txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"where does winston live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567e0e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9685850> JSON: {\n",
       "  \"prompt_tokens\": 2122,\n",
       "  \"completion_tokens\": 20,\n",
       "  \"total_tokens\": 2142,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-76b78b5e-d6bd-4521-8fd7-8b04bb44edb2-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "         MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "def load_memory(_):\n",
    "    x = memory.load_memory_variables({})\n",
    "    return x[\"history\"]\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(history=load_memory)\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Is Aaronson guilty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e11ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='He wrote \"2+2=5\" in the dust on the table.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9603170> JSON: {\n",
       "  \"prompt_tokens\": 2220,\n",
       "  \"completion_tokens\": 16,\n",
       "  \"total_tokens\": 2236,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8e510fa9-bdce-49eb-9af7-8ed9bc7905a6-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What message did he write in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303cbfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Julia is a character mentioned in the text. She is someone who Winston loves and cares for deeply, and he expresses a strong desire to protect her from harm.', additional_kwargs={}, response_metadata={'token_usage': <OpenAIObject at 0x1e3a9684b30> JSON: {\n",
       "  \"prompt_tokens\": 2074,\n",
       "  \"completion_tokens\": 33,\n",
       "  \"total_tokens\": 2107,\n",
       "  \"prompt_tokens_details\": {\n",
       "    \"cached_tokens\": 0,\n",
       "    \"audio_tokens\": 0\n",
       "  },\n",
       "  \"completion_tokens_details\": {\n",
       "    \"reasoning_tokens\": 0,\n",
       "    \"audio_tokens\": 0,\n",
       "    \"accepted_prediction_tokens\": 0,\n",
       "    \"rejected_prediction_tokens\": 0\n",
       "  }\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-83fef15b-5c91-44a0-b7e5-cf2180c26534-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is Julia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a93d0305233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "                \"documents\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            } | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer.\n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d15290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind( # auto로 주면 알아서 결정해서 씀 내가 만든 걸 쓸 수도 있고 아닐 수도 있음\n",
    "    function_call={\n",
    "        \"name\": \"create_quiz\",\n",
    "    },\n",
    "    functions=[\n",
    "        function,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "response"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=[\n",
    "        StructuredTool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. This tool take two arguments, both  should be numbers.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "555f75e51314322e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "def plus(inputs):\n",
    "    a, b = inputs.split(\",\")\n",
    "    return float(a) + float(b)\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        Tool.from_function(\n",
    "            func=plus,\n",
    "            name=\"Sum Calculator\",\n",
    "            description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample:1,2\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "90e5c3f784946479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float = Field(description=\"The first number\")\n",
    "    b: float = Field(description=\"The second number\")\n",
    "\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"CalculatorTool\"\n",
    "    description = \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[CalculatorToolArgsSchema] = CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    "    tools=[\n",
    "        CalculatorTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $355.39 + $924.87 + $721.2 + $1940.29 + $573.63 + $65.72 + $35.00 + $552.00 + $76.16 + $29.12\"\n",
    "\n",
    "agent.invoke(prompt)"
   ],
   "id": "1ff01a749e2e815d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
